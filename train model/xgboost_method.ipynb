{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as py\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split #use to split train-data and test-data\n",
    "\n",
    "TRAIN_FILE='predict_feactures_lable.csv'\n",
    "PREDICT_FILE='predict_data.csv'\n",
    "RESULT=\"predict_result_logistic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取数据,输入文件的地址和需要的字段  （因为内存有限，而且有的时候我们没有必要将所有字段都读取）\n",
    "def get_date(fname):\n",
    "    #因为数据量较大，一次读入可能会导致内存错误，所以使用pandas的分块chunk进行读取\n",
    "    reader=pd.read_csv(fname,header=0,iterator=True)#后面的iterator=True是在喊“我要分块读，你不要把我一次读完”\n",
    "    chunkSize=100000\n",
    "    chunks=[]\n",
    "    loop=True\n",
    "    while loop:\n",
    "        try:\n",
    "            chunk=reader.get_chunk(chunkSize)\n",
    "            chunks.append(chunk)\n",
    "        except StopIteration:\n",
    "            loop=False\n",
    "            print(\"Iteration is stopped\")\n",
    "    df_ac=pd.concat(chunks,ignore_index=True)\n",
    "    #df_ac=df_ac.drop_duplicates() #所以原来的程序这里是不对的，应该是所有的都相同才删除吧\n",
    "    #这个应该也是不要去重的\n",
    "    return df_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取数据,输入文件的地址和需要的字段  （因为内存有限，而且有的时候我们没有必要将所有字段都读取）\n",
    "def get_date_needlist(fname,rowlist):\n",
    "    #因为数据量较大，一次读入可能会导致内存错误，所以使用pandas的分块chunk进行读取\n",
    "    reader=pd.read_csv(fname,header=0,iterator=True)#后面的iterator=True是在喊“我要分块读，你不要把我一次读完”\n",
    "    chunkSize=100000\n",
    "    chunks=[]\n",
    "    loop=True\n",
    "    while loop:\n",
    "        try:\n",
    "            chunk=reader.get_chunk(chunkSize)[rowlist]\n",
    "            chunks.append(chunk)\n",
    "        except StopIteration:\n",
    "            loop=False\n",
    "            print(\"Iteration is stopped\")\n",
    "    df_ac=pd.concat(chunks,ignore_index=True)\n",
    "    return df_ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_xgboost_model():\n",
    "    #get train-data and text-date\n",
    "    X=get_date(TRAIN_FILE)\n",
    "    X= X.dropna(how = 'any', axis = 0)#如果有缺失数据则删除\n",
    "    y=X['buy']\n",
    "    del X['buy']\n",
    "    #split train-data and text-data\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "   \n",
    "    #use xgboost\n",
    "    dtrain=xgb.DMatrix(X_train,label=y_train)\n",
    "    dtext=xgb.DMatrix(X_test,label=y_test) #it really need different data to test the data\n",
    "    \n",
    "    param={'booster':'gbtree','objective':'binary:logistic','eta':0.1.'max_depth':10,'subsample':1.0,/\n",
    "          'min_child_weight':5,'colsample_bytree':0.2,'scale_pos_weight':0.1,'eval_metric':'auc','gamma':0.2,'lambda':300}\n",
    "    \n",
    "    watchlist=[(dtrain,'train'),(dtext,'val')]\n",
    "    \n",
    "    print('begain training')\n",
    "    model=xgb.train(param,dtrain,10,evals=watchlist)#num_boost_round=1\n",
    "    print('training end')\n",
    "    \n",
    "    #save the model\n",
    "    model.save_model(\"xgboost001.model\")\n",
    "    \n",
    "    \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def useModelPredict():\n",
    "        #get predict-data\n",
    "        X_pre=get_date(PREDICT_FILE) \n",
    "        X_pre=X_pre.drop_duplicates(['user_id','sku_id'])\n",
    "        X_pre = X_pre.dropna(how = 'any', axis = 0)#如果有缺失数据则删除   \n",
    "        \n",
    "        #load the model\n",
    "        bst=xgb.Booser(model_file=\"xgboost001.model\")\n",
    "        \n",
    "        #use the model to predict\n",
    "        y_pre=bst.predict(X_pre)\n",
    "        result=X_pre([['user_id','sku_id']])\n",
    "        result['pro']=pd.Serise(y_pre) #buy-probability of every user-product\n",
    "        \n",
    "        #结果只需要提交预测为购买的用户，不购买的无需提交\n",
    "        result=result[result['pro']>0.5]\n",
    "        #如果一个用户被同时预测会购买多种产品，则只提交最最可能购买的那个产品\n",
    "        idx=result.groupby(['user_id'])['pro'].transform(max)==result['pro']\n",
    "        result=result[idx]\n",
    "        #此时已然会存在一个用户被预测购买多种产品的情形，即概率值相同被同时保存，智能保留一个user_id\n",
    "        result.drop_duplicates(['user_id'],inplace=True)\n",
    "        #使user全部是user表中的（本来就是，不用管），product全部是product表中的\n",
    "        df_pro=get_date_needlist('JData_inter_product.csv', ['sku_id'])\n",
    "        result=result.merge(df_pro,on='sku_id',how='inner')\n",
    "        \n",
    "        #save the result\n",
    "        result.to_csv(RESULT, index = False)\n",
    "        print('end') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
